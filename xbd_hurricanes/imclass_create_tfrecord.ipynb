{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imclass_create_tfrecord.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ialnwURlETfF",
        "outputId": "de928ffd-616d-412c-a14c-6511e9198c4a"
      },
      "source": [
        "# Written by Dr Daniel Buscombe, Marda Science LLC\n",
        "#\n",
        "# MIT License\n",
        "#\n",
        "# Copyright (c) 2020, Marda Science LLC\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "\n",
        "import numpy as np\n",
        "from imageio import imread, imwrite\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from numpy.lib.stride_tricks import as_strided as ast\n",
        "import random, string, os\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "\n",
        "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "\n",
        "##calcs\n",
        "import tensorflow as tf #numerical operations on gpu\n",
        "\n",
        "SEED=42\n",
        "np.random.seed(SEED)\n",
        "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
        "\n",
        "\n",
        "ims_per_shard = 200\n",
        "\n",
        "###==========================================================================\n",
        "\n",
        "def writeout_tfrecords(storm):\n",
        "    # for storm in ['matthew', 'michael', 'florence', 'harvey']:\n",
        "    if storm=='matthew':\n",
        "        n=842\n",
        "    elif storm=='michael':\n",
        "        n=1057\n",
        "    elif storm=='florence':\n",
        "        n=586\n",
        "    elif storm=='harvey':\n",
        "        n=1172\n",
        "\n",
        "    print('Working on %s' % (storm))\n",
        "    imdir = '/media/marda/TWOTB1/xBD/hurricanes/tiled_images/'+storm\n",
        "    tfrecord_dir = '/media/marda/TWOTB1/xBD/hurricanes/tfrecords/'+storm+'/imrecog'\n",
        "\n",
        "    nb_images=len(glob(imdir+os.sep+'destroyed/*.jpg'))+len(glob(imdir+os.sep+'no-damage/*.jpg'))+\\\n",
        "        len(glob(imdir+os.sep+'minor-damage/*.jpg'))+len(glob(imdir+os.sep+'major-damage/*.jpg'))+len(glob(imdir+os.sep+'un-classified/*.jpg'))\n",
        "    print('Image tiles: %i' % (nb_images))\n",
        "\n",
        "    SHARDS = int(nb_images / ims_per_shard) + (1 if nb_images % ims_per_shard != 0 else 0)\n",
        "    print('tfrecord shards: %i' % (SHARDS))\n",
        "\n",
        "    shared_size = int(np.ceil(1.0 * nb_images / SHARDS))\n",
        "\n",
        "    all_images=glob(imdir+os.sep+'destroyed/*.jpg')+glob(imdir+os.sep+'no-damage/*.jpg')+\\\n",
        "        glob(imdir+os.sep+'minor-damage/*.jpg')+glob(imdir+os.sep+'major-damage/*.jpg')+glob(imdir+os.sep+'un-classified/*.jpg')\n",
        "\n",
        "    for k in range(10):\n",
        "        random.shuffle(all_images)\n",
        "    Z,_ = sliding_window(np.array(all_images), (shared_size), (shared_size))\n",
        "\n",
        "    for counter in range(n,len(Z)):\n",
        "        try:\n",
        "            print('%i out of %i' % (counter, len(Z)))\n",
        "            dataset = tf.data.Dataset.list_files(Z[counter], shuffle=None) #imdir+os.sep+'destroyed/*.jpg',\n",
        "            dataset = get_recog_dataset_for_tfrecords(dataset, shared_size)\n",
        "            write_records(dataset, tfrecord_dir, types, counter)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "#-----------------------------------\n",
        "\"\"\"\n",
        "These functions cast inputs into tf dataset 'feature' classes\n",
        "There is one for bytestrings (images), one for floats (not used here) and one for ints (labels)\n",
        "\"\"\"\n",
        "def _bytestring_feature(list_of_bytestrings):\n",
        "    \"\"\"\n",
        "    \"_bytestring_feature(list_of_bytestrings)\"\n",
        "    cast inputs into tf dataset 'feature' classes\n",
        "    INPUTS:\n",
        "        * list_of_bytestrings\n",
        "    OPTIONAL INPUTS:\n",
        "    GLOBAL INPUTS:\n",
        "    OUTPUTS: tf.train.Feature example\n",
        "    \"\"\"\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "def _int_feature(list_of_ints):\n",
        "    \"\"\"\n",
        "    \"_int_feature(list_of_ints)\"\n",
        "    cast inputs into tf dataset 'feature' classes\n",
        "    INPUTS:\n",
        "        * list_of_ints\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: None\n",
        "    OUTPUTS: tf.train.Feature example\n",
        "    \"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
        "\n",
        "def _float_feature(list_of_floats):\n",
        "    \"\"\"\n",
        "    \"_float_feature(list_of_floats)\"\n",
        "    cast inputs into tf dataset 'feature' classes\n",
        "    INPUTS:\n",
        "        * list_of_floats\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: None\n",
        "    OUTPUTS: tf.train.Feature example\n",
        "    \"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
        "\n",
        "#-----------------------------------\n",
        "def to_tfrecord(img_bytes, label, types):\n",
        "    \"\"\"\n",
        "    to_tfrecord(img_bytes, label, types)\n",
        "    This function creates a TFRecord example from an image byte string and a label feature\n",
        "    INPUTS:\n",
        "        * img_bytes: an image bytestring\n",
        "        * label: label string of image\n",
        "        * types: list of string classes in the entire dataset\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: None\n",
        "    OUTPUTS: tf.train.Feature example\n",
        "    \"\"\"\n",
        "    class_num = np.argmax(np.array(types)==label)\n",
        "    feature = {\n",
        "      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
        "      \"class\": _int_feature([class_num]),        # one class in the list\n",
        "              }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
        "   return ''.join(random.choice(chars) for _ in range(size))\n",
        "\n",
        "# =========================================================\n",
        "def norm_shape(shap):\n",
        "   '''\n",
        "   Normalize numpy array shapes so they're always expressed as a tuple,\n",
        "   even for one-dimensional shapes.\n",
        "   '''\n",
        "   try:\n",
        "      i = int(shap)\n",
        "      return (i,)\n",
        "   except TypeError:\n",
        "      # shape was not a number\n",
        "      pass\n",
        "\n",
        "   try:\n",
        "      t = tuple(shap)\n",
        "      return t\n",
        "   except TypeError:\n",
        "      # shape was not iterable\n",
        "      pass\n",
        "\n",
        "   raise TypeError('shape must be an int, or a tuple of ints')\n",
        "\n",
        "# =========================================================\n",
        "# Return a sliding window over a in any number of dimensions\n",
        "# version with no memory mapping\n",
        "def sliding_window(a,ws,ss = None,flatten = True):\n",
        "    '''\n",
        "    Return a sliding window over a in any number of dimensions\n",
        "    '''\n",
        "    if None is ss:\n",
        "        # ss was not provided. the windows will not overlap in any direction.\n",
        "        ss = ws\n",
        "    ws = norm_shape(ws)\n",
        "    ss = norm_shape(ss)\n",
        "    # convert ws, ss, and a.shape to numpy arrays\n",
        "    ws = np.array(ws)\n",
        "    ss = np.array(ss)\n",
        "    shap = np.array(a.shape)\n",
        "    # ensure that ws, ss, and a.shape all have the same number of dimensions\n",
        "    ls = [len(shap),len(ws),len(ss)]\n",
        "    if 1 != len(set(ls)):\n",
        "        raise ValueError(\\\n",
        "        'a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n",
        "\n",
        "    # ensure that ws is smaller than a in every dimension\n",
        "    if np.any(ws > shap):\n",
        "        raise ValueError(\\\n",
        "        'ws cannot be larger than a in any dimension.\\\n",
        " a.shape was %s and ws was %s' % (str(a.shape),str(ws)))\n",
        "    # how many slices will there be in each dimension?\n",
        "    newshape = norm_shape(((shap - ws) // ss) + 1)\n",
        "    # the shape of the strided array will be the number of slices in each dimension\n",
        "    # plus the shape of the window (tuple addition)\n",
        "    newshape += norm_shape(ws)\n",
        "    # the strides tuple will be the array's strides multiplied by step size, plus\n",
        "    # the array's strides (tuple addition)\n",
        "    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n",
        "    a = ast(a,shape = newshape,strides = newstrides)\n",
        "    if not flatten:\n",
        "        return a\n",
        "    # Collapse strided so that it has one more dimension than the window.  I.e.,\n",
        "    # the new array is a flat list of slices.\n",
        "    meat = len(ws) if ws.shape else 0\n",
        "    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n",
        "    dim = firstdim + (newshape[-meat:])\n",
        "    # remove any dimensions with size 1\n",
        "    #dim = filter(lambda i : i != 1,dim)\n",
        "\n",
        "    return a.reshape(dim), newshape\n",
        "\n",
        "# =========================================================\n",
        "def writeout(tmp, cl, labels, outpath, thres):\n",
        "\n",
        "    #l, cnt = md(cl.flatten())\n",
        "    #l = np.squeeze(l)\n",
        "    #if l==0:\n",
        "\n",
        "    dist = np.bincount(cl.flatten(), minlength=len(labels))\n",
        "    if np.all(dist[1:]==0)==True:\n",
        "        l=0\n",
        "        cnt = np.max(dist)\n",
        "    else:\n",
        "        l=np.argmax(dist[1:])+1\n",
        "        cnt = np.max(dist[1:])\n",
        "\n",
        "    if cnt/len(cl.flatten()) > thres:\n",
        "        outfile = id_generator()+'.jpg'\n",
        "        try:\n",
        "            fp = outpath+os.sep+labels[l]+os.sep+outfile\n",
        "            imwrite(fp, tmp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "#-----------------------------------\n",
        "def to_tfrecord(img_bytes, label, CLASSES):\n",
        "    \"\"\"\n",
        "    to_tfrecord(img_bytes, label, CLASSES)\n",
        "    This function creates a TFRecord example from an image byte string and a label feature\n",
        "    INPUTS:\n",
        "        * img_bytes: an image bytestring\n",
        "        * label: label string of image\n",
        "        * CLASSES: list of string classes in the entire dataset\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: None\n",
        "    OUTPUTS: tf.train.Feature example\n",
        "    \"\"\"\n",
        "    class_num = np.argmax(np.array(CLASSES)==label)\n",
        "    feature = {\n",
        "      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
        "      \"class\": _int_feature([class_num]),        # one class in the list\n",
        "              }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "#-----------------------------------\n",
        "def read_image_and_label(img_path):\n",
        "    \"\"\"\n",
        "    read_image_and_label(img_path)\n",
        "    This function reads a jpeg image from a provided filepath\n",
        "    and extracts the label from the filename (assuming the class name is\n",
        "    before \"_IMG\" in the filename)\n",
        "    INPUTS:\n",
        "        * img_path [string]: filepath to a jpeg image\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * image [tensor array]\n",
        "        * class_label [tensor int]\n",
        "    \"\"\"\n",
        "    bits = tf.io.read_file(img_path)\n",
        "    image = tf.image.decode_jpeg(bits)\n",
        "\n",
        "    label = tf.strings.split(img_path, sep='/')\n",
        "    #label = tf.strings.split(label[0], sep='_IMG')\n",
        "\n",
        "    return image,label[-2]\n",
        "\n",
        "#-----------------------------------\n",
        "def resize_and_crop_image(image, label):\n",
        "    \"\"\"\n",
        "    resize_and_crop_image(image, label)\n",
        "    This function crops to square and resizes an image\n",
        "    The label passes through unmodified\n",
        "    INPUTS:\n",
        "        * image [tensor array]\n",
        "        * label [int]\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: TARGET_SIZE\n",
        "    OUTPUTS:\n",
        "        * image [tensor array]\n",
        "        * label [int]\n",
        "    \"\"\"\n",
        "    w = tf.shape(image)[0]\n",
        "    h = tf.shape(image)[1]\n",
        "    tw = TARGET_SIZE\n",
        "    th = TARGET_SIZE\n",
        "    resize_crit = (w * th) / (h * tw)\n",
        "    image = tf.cond(resize_crit < 1,\n",
        "                  lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
        "                  lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
        "                 )\n",
        "    nw = tf.shape(image)[0]\n",
        "    nh = tf.shape(image)[1]\n",
        "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
        "    return image, label\n",
        "\n",
        "#-----------------------------------\n",
        "def recompress_image(image, label):\n",
        "    \"\"\"\n",
        "    recompress_image(image, label)\n",
        "    This function takes an image encoded as a byte string\n",
        "    and recodes as an 8-bit jpeg\n",
        "    Label passes through unmodified\n",
        "    INPUTS:\n",
        "        * image [tensor array]\n",
        "        * label [int]\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: None\n",
        "    OUTPUTS:\n",
        "        * image [tensor array]\n",
        "        * label [int]\n",
        "    \"\"\"\n",
        "    image = tf.cast(image, tf.uint8)\n",
        "    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
        "    return image, label\n",
        "\n",
        "#-----------------------------------\n",
        "def get_recog_dataset_for_tfrecords(dataset, shared_size):\n",
        "    \"\"\"\n",
        "    \"get_recog_dataset_for_tfrecords\"\n",
        "    This function reads an image and label and decodes both jpegs\n",
        "    into bytestring arrays.\n",
        "    This is the version for data, which differs in use of both\n",
        "    resize_and_crop_seg_image and resize_and_crop_seg_image\n",
        "    for image pre-processing\n",
        "    INPUTS:\n",
        "        * image [tensor array]\n",
        "        * label [tensor array]\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: TARGET_SIZE\n",
        "    OUTPUTS:\n",
        "        * image [tensor array]\n",
        "        * label [tensor array]\n",
        "    \"\"\"\n",
        "    dataset = dataset.map(read_image_and_label)\n",
        "    dataset = dataset.map(resize_and_crop_image, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(recompress_image, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.batch(shared_size)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#-----------------------------------\n",
        "def write_records(tamucc_dataset, tfrecord_dir, types, counter):\n",
        "    \"\"\"\n",
        "    write_records(tamucc_dataset, tfrecord_dir, types)\n",
        "    This function writes a tf.data.Dataset object to TFRecord shards\n",
        "    INPUTS:\n",
        "        * tamucc_dataset [tf.data.Dataset]\n",
        "        * tfrecord_dir [string] : path to directory where files will be written\n",
        "        * CLASSES [list] of class string names\n",
        "    OPTIONAL INPUTS: None\n",
        "    GLOBAL INPUTS: None\n",
        "    OUTPUTS: None (files written to disk)\n",
        "    \"\"\"\n",
        "    for shard, (image, label) in enumerate(tamucc_dataset):\n",
        "      shard_size = image.numpy().shape[0]\n",
        "      filename = tfrecord_dir+os.sep+\"xbDhurricanes-\"+str(counter) + \"-{:02d}-{}.tfrec\".format(shard, shard_size)\n",
        "\n",
        "      with tf.io.TFRecordWriter(filename) as out_file:\n",
        "        for i in range(shard_size):\n",
        "          example = to_tfrecord(image.numpy()[i],label.numpy()[i], types)\n",
        "          out_file.write(example.SerializeToString())\n",
        "        print(\"Wrote file {} containing {} records\".format(filename, shard_size))\n",
        "\n",
        "\n",
        "###############################################################\n",
        "## VARIABLES\n",
        "###############################################################\n",
        "damage_dict = {\n",
        "    \"no-damage\": (0, 255, 0),\n",
        "    \"minor-damage\": (0, 0, 255),\n",
        "    \"major-damage\": (255, 69, 0),\n",
        "    \"destroyed\": (255, 0, 0),\n",
        "    \"un-classified\": (255, 255, 255)\n",
        "}\n",
        "cols = [damage_dict[k] for k in damage_dict]\n",
        "\n",
        "types = [k for k in damage_dict]\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "TARGET_SIZE = tile = 96\n",
        "thres = 0.25\n",
        "\n",
        "# for storm in ['matthew', 'michael', 'florence', 'harvey']:\n",
        "#     print('Working on %s' % (storm))\n",
        "#     imdir = '/media/marda/TWOTB1/xBD/hurricanes/images/'+storm\n",
        "#     lab_path =  '/media/marda/TWOTB1/xBD/hurricanes/labels2D/'+storm\n",
        "#     outpath = '/media/marda/TWOTB1/xBD/hurricanes/tiled_images/'+storm\n",
        "#\n",
        "#     images = sorted(glob(imdir+os.sep+'*.png'))\n",
        "#     labels = sorted(glob(lab_path+os.sep+'*.png'))\n",
        "#\n",
        "#     for i,l in zip(images, labels):\n",
        "#         Z,_ = sliding_window(imread(i), (tile,tile,3), (int(tile/2), int(tile/2),3))\n",
        "#\n",
        "#         C,_ = sliding_window(imread(l), (tile,tile), (int(tile/2), int(tile/2)))\n",
        "#\n",
        "#         Parallel(n_jobs=-1, verbose=0)(delayed(writeout)(Z[k], C[k], types, outpath, thres) for k in range(len(Z)))\n",
        "#         del Z, C\n",
        "#     del images, labels\n",
        "\n",
        "\n",
        "Parallel(n_jobs=-1, verbose=0)(delayed(writeout_tfrecords)(storm) for storm in ['matthew', 'michael', 'florence', 'harvey'])\n",
        "\n",
        "# storm='matthew'\n",
        "#\n",
        "# writeout_tfrecords(storm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#==\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  2.7.0\n",
            "Eager mode:  True\n",
            "GPU name:  []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cf59f4898ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriteout_tfrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstorm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'matthew'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'michael'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'florence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'harvey'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;31m# storm='matthew'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    }
  ]
}